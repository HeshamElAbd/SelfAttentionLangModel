# SelfAttentionLangModel
Language model based upon the Encoder units of the transformer. For Theortical back ground please refere to Attention is all you need 
paper @(https://arxiv.org/abs/1706.03762) and for detials regard the impelementation please refere to the source code here and to
google tutorial avilable at https://www.tensorflow.org/beta/tutorials/text/transformer#scaled_dot_product_attention. 

## To DO: 
### Add a custom Training Function for the modeler to manage return attention weight case. 

## Current State: 
### The modeler Model is ready for deployment as a TF models with return_attention_Weight=False
