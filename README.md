# SelfAttentionLangModel
Language model based upon the Encoder units of the transformer. For Theortical back ground please refere to Attention is all you need 
paper @(https://arxiv.org/abs/1706.03762) and for detials regard the impelementation please refere to the source code here and to
google tutorial avilable at https://www.tensorflow.org/beta/tutorials/text/transformer#scaled_dot_product_attention. 

